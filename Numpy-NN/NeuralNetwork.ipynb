{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0) * x \n",
    "\n",
    "def relu_grad(x):\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetlights = np.array([[1,0,1], \n",
    "                         [0,1,1], \n",
    "                         [0,0,1], \n",
    "                         [1,1,1], \n",
    "                         [0,1,1], \n",
    "                         [1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_vs_stop = np.array([[0], [1], [0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = streetlights, walk_vs_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_nodes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  # number of iterations to go through the network\n",
    "\n",
    "lr = 0.01      # how much we change the weights of the network each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, y.shape[1]) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.019496, Accuracy = 50.00%\n",
      "Epoch 10: Error = 0.046335, Accuracy = 83.33%\n",
      "Epoch 20: Error = 0.063009, Accuracy = 100.00%\n",
      "Epoch 30: Error = 0.062283, Accuracy = 100.00%\n",
      "Epoch 40: Error = 0.052676, Accuracy = 100.00%\n",
      "Epoch 50: Error = 0.041539, Accuracy = 100.00%\n",
      "Epoch 60: Error = 0.031829, Accuracy = 100.00%\n",
      "Epoch 70: Error = 0.0242, Accuracy = 100.00%\n",
      "Epoch 80: Error = 0.018454, Accuracy = 100.00%\n",
      "Epoch 90: Error = 0.015026, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = relu(layer_in.dot(ws_1))\n",
    "        layer_out = layer_1.dot(ws_2)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_2 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # Update weights\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "\n",
    "        #calc the the error each node in prev layer contributed\n",
    "        delta_1 = delta_2.dot(ws_2.T) * relu_grad(layer_1)\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "        \n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = correct_predictions / X.shape[0]\n",
    "        error = delta_2 ** 2\n",
    "        print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing ReLU activation function with Sigmoid activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 1e-05, Accuracy = 50.00%\n",
      "Epoch 10: Error = 0.118293, Accuracy = 50.00%\n",
      "Epoch 20: Error = 0.195965, Accuracy = 66.67%\n",
      "Epoch 30: Error = 0.217885, Accuracy = 100.00%\n",
      "Epoch 40: Error = 0.21912, Accuracy = 100.00%\n",
      "Epoch 50: Error = 0.21389, Accuracy = 100.00%\n",
      "Epoch 60: Error = 0.206761, Accuracy = 100.00%\n",
      "Epoch 70: Error = 0.199111, Accuracy = 100.00%\n",
      "Epoch 80: Error = 0.191344, Accuracy = 100.00%\n",
      "Epoch 90: Error = 0.183578, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, y.shape[1]) - 0.5\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = sigmoid(layer_in.dot(ws_1))\n",
    "        layer_out = layer_1.dot(ws_2)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_2 = layer_out - y[i:i+1]\n",
    "        \n",
    "        # Update weights\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        delta_1 = delta_2.dot(ws_2.T) * sigmoid_grad(layer_1)\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "        \n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = correct_predictions / X.shape[0]\n",
    "        error = delta_2 ** 2\n",
    "        print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, when using the ReLU function, the accuracy is 50%  in the beginning, before it gradually increases and quickly converges towards 100%. On the other hand, the model seems to require more epochs to converge when using the Sigmoid function - both converge towards 100%\n",
    "\n",
    "There could be several reasons for the observation described above to occur. One reason could be that the sigmoid activation function is much more sensitive to the initial weights and learning rate than the ReLU function is. Also, one can experience the vanishing gradient problem where the perceptron training rule does little to change the \"update term\" being extremely small - the sigmoid activation function is susceptible to this problem due to the gradient never going higher than 0.25 as seen in the graph below, thus requiring more epochs to converge.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*6A3A_rt4YmumHusvTvVTxw.png\"></img>\n",
    "\n",
    "Image source: https://miro.medium.com/v2/resize:fit:1400/1*6A3A_rt4YmumHusvTvVTxw.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning rates and epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Reached 100% accuracy at epoch 116 \n",
      "\n",
      "Learning rate: 0.01\n",
      "Reached 100% accuracy at epoch 12 \n",
      "\n",
      "Learning rate: 0.1\n",
      "Reached 100% accuracy at epoch 1 \n",
      "\n",
      "Learning rate: 1\n",
      "Does not converge after 120 epochs\n",
      "\n",
      "Learning rate: 10\n",
      "Does not converge after 120 epochs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikael\\AppData\\Local\\Temp\\ipykernel_13456\\3626363342.py:32: RuntimeWarning: overflow encountered in square\n",
      "  error = delta_2 ** 2\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 1, 10]\n",
    "epochs = 120\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    np.random.seed(1)\n",
    "    ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "    ws_2 = np.random.rand(hidden_nodes, y.shape[1]) - 0.5\n",
    "\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            layer_in = X[i:i+1]\n",
    "            \n",
    "            # Forward pass/prediction\n",
    "            layer_1 = relu(layer_in.dot(ws_1))\n",
    "            layer_out = layer_1.dot(ws_2)\n",
    "            \n",
    "            #calc error/distance (how far are we from goal)\n",
    "            delta_2 = layer_out - y[i:i+1]\n",
    "            \n",
    "            # Update weights\n",
    "            ws_2 -= learning_rate * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "            delta_1 = delta_2.dot(ws_2.T) * relu_grad(layer_1)\n",
    "            ws_1 -= learning_rate * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "            \n",
    "            if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / X.shape[0]\n",
    "        error = delta_2 ** 2\n",
    "        if accuracy == 1.0:\n",
    "            print(\"Reached 100% accuracy at epoch\", epoch, \"\\n\")\n",
    "            break\n",
    "        if epoch == epochs - 1:\n",
    "            print(f\"Does not converge after {epochs} epochs\\n\")\n",
    "\n",
    "        correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experimenting with the necessary epochs to find the best learning rate, it was found that among the learning rates that enable the model to converge, the learning rate 0.001 required the most epochs (116), therefore setting epochs to 120 seems like a reasonable choice.\n",
    "\n",
    "The largest learning rates (1 and 10) does not converge at all. In most cases a learning rate greater than 1 is not advisable, due to the risk of overshooting, fluctuation and divergence - as demonstrated in this case.\n",
    "\n",
    "The best learning rate (0.1) reaches 100% accuracy already at 1 epoch and is picked as the preferred learning rate proceeding forward.\n",
    "\n",
    "It is worth noting that the observations above would be different if we proceeded with using the Sigmoid activation function instead of the ReLU function, here I have chosen to go back to the ReLU function as it seems to increase the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding another hidden layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the learning rate has been set to the optimal value found previously and epochs has been set to 5 as this is more than enough to illustrate further improvements of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.003139, Accuracy = 50.00%\n",
      "Epoch 1: Error = 0.016075, Accuracy = 50.00%\n",
      "Epoch 2: Error = 0.050922, Accuracy = 50.00%\n",
      "Epoch 3: Error = 0.08964, Accuracy = 100.00%\n",
      "Epoch 4: Error = 0.096032, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "hidden_nodes_2 = 6\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "\n",
    "correct_predictions = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = relu(layer_in.dot(ws_1))\n",
    "        layer_2 = relu(layer_1.dot(ws_2))\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        delta_2 = delta_3.dot(ws_3.T) * relu_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * relu_grad(layer_1)\n",
    "        \n",
    "        # Update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "        \n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    error = delta_3 ** 2\n",
    "    print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another hidden layer seems to make the network less performant. This could be due to the fact that the problem that is attempted to be modelled, is a very simple one, hence increasing the model complexity might cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the effect of the activation function**\n",
    "\n",
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.014366, Accuracy = 50.00%\n",
      "Epoch 1: Error = 0.050342, Accuracy = 50.00%\n",
      "Epoch 2: Error = 0.091095, Accuracy = 100.00%\n",
      "Epoch 3: Error = 0.093618, Accuracy = 100.00%\n",
      "Epoch 4: Error = 0.075225, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "hidden_nodes_2 = 6\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "\n",
    "correct_predictions = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = relu(layer_in.dot(ws_1))\n",
    "        layer_2 = layer_1.dot(ws_2)\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        delta_2 = delta_3.dot(ws_3.T) \n",
    "        delta_1 = delta_2.dot(ws_2.T) * relu_grad(layer_1)\n",
    "        \n",
    "        # Update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "        \n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    error = delta_3 ** 2\n",
    "    print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the activation function in the first hidden layer gives the model a degree of non-linearity, which in turn would be beneficial if the data that is to be modelled was complex. In this case it does not seem necessary as demonstrated in (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.00423, Accuracy = 50.00%\n",
      "Epoch 1: Error = 0.064511, Accuracy = 83.33%\n",
      "Epoch 2: Error = 0.07154, Accuracy = 100.00%\n",
      "Epoch 3: Error = 0.043882, Accuracy = 100.00%\n",
      "Epoch 4: Error = 0.022942, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "hidden_nodes_2 = 6\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "\n",
    "correct_predictions = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = layer_in.dot(ws_1)\n",
    "        layer_2 = relu(layer_1.dot(ws_2))\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        delta_2 = delta_3.dot(ws_3.T) * relu_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) \n",
    "        \n",
    "        # Update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    error = delta_3 ** 2\n",
    "    print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the problem at hand being relatively simple without the raw input sporting a lot of complex relationships, it seems that keeping the first layer linear is okay due to the second layer introducing sufficient non-linearity. Out of the three alternatives, (b) gives the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.003139, Accuracy = 50.00%\n",
      "Epoch 1: Error = 0.016075, Accuracy = 50.00%\n",
      "Epoch 2: Error = 0.050922, Accuracy = 50.00%\n",
      "Epoch 3: Error = 0.08964, Accuracy = 100.00%\n",
      "Epoch 4: Error = 0.096032, Accuracy = 100.00%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "hidden_nodes_2 = 6\n",
    "ws_1 = np.random.rand(X.shape[1], hidden_nodes) - 0.5\n",
    "ws_2 = np.random.rand(hidden_nodes, hidden_nodes_2) - 0.5\n",
    "ws_3 = np.random.rand(hidden_nodes_2, y.shape[1]) - 0.5\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "\n",
    "correct_predictions = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "        layer_in = X[i:i+1]\n",
    "        \n",
    "        # Forward pass/prediction\n",
    "        layer_1 = relu(layer_in.dot(ws_1))\n",
    "        layer_2 = relu(layer_1.dot(ws_2))\n",
    "        layer_out = layer_2.dot(ws_3)\n",
    "        \n",
    "        #calc error/distance (how far are we from goal)\n",
    "        delta_3 = layer_out - y[i:i+1]\n",
    "        delta_2 = delta_3.dot(ws_3.T) * relu_grad(layer_2)\n",
    "        delta_1 = delta_2.dot(ws_2.T) * relu_grad(layer_1)\n",
    "        \n",
    "        # Update weights\n",
    "        ws_3 -= lr * (layer_2.T.reshape(hidden_nodes_2, 1).dot(delta_3))\n",
    "        ws_2 -= lr * (layer_1.T.reshape(hidden_nodes, 1).dot(delta_2))\n",
    "        ws_1 -= lr * (layer_in.T.reshape(X.shape[1], 1).dot(delta_1))\n",
    "\n",
    "        if abs(layer_out[0][0] - y[i:i+1][0][0]) < 0.5:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / X.shape[0]\n",
    "    error = delta_3 ** 2\n",
    "    print(f\"Epoch {epoch}: Error = {round(error[0][0], 6)}, Accuracy = {accuracy * 100:.2f}%\")\n",
    "    correct_predictions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option (c) gives the highest level of non-linearity which causes the model to be able to capture complex relationships in the data. In this case however, it leads to decreased performance due to the problem at hand being relatively simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the models above, one could potentially reduce the number of weight parameters by reducing the amount of neurons in each of the hidden layers (as long as it is not done excessively). One could also implement weight sharing for the first or second layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
