{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43fc085a-8842-4326-9042-68384e45be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "780       0\n",
      "781       0\n",
      "782       0\n",
      "783       0\n",
      "labels    2\n",
      "Name: 199, Length: 785, dtype: int64\n",
      "Validation Accuracy of the model: 100.00%\n",
      "Validation Accuracy of the model: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Data loader\n",
    "\n",
    "1. Read the data from the CSV file\n",
    "    We'll use the pandas library to read the CSV file and then transform the data into PyTorch tensors.\n",
    "\n",
    "2. Splitting the dataset\n",
    "    We'll use the train_test_split function from sklearn to split the data into training, validation, and test sets. For demonstration purposes, we'll split the data as follows:\n",
    "\n",
    "    70% Training\n",
    "    15% Validation\n",
    "    15% Test\n",
    "\n",
    "3. Creating Dataloaders\n",
    "    Once we've split the data, we'll convert each set into a PyTorch TensorDataset and then use the DataLoader class to create loaders for each set.\n",
    "\"\"\"\n",
    "# 1. Read data from the CSV file\n",
    "data = pd.read_csv('MNIST_dataset.csv')\n",
    "\n",
    "labels = torch.tensor(data.iloc[:, 0].values, dtype=torch.long)\n",
    "pixels = data.iloc[:, 1:].values\n",
    "\n",
    "channels     = 1\n",
    "image_height = 28\n",
    "image_width  = 28\n",
    "\n",
    "pixels_array = pixels.reshape(-1, channels, image_height, image_width)\n",
    "\n",
    "images = torch.tensor(pixels_array, dtype=torch.float32) / 255.0\n",
    "\n",
    "training_split = 0.7\n",
    "validation_split = 0.1\n",
    "test_split = 0.2\n",
    "\n",
    "first_split = test_split + validation_split\n",
    "second_split = test_split / (validation_split + test_split)\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(images, labels, test_size=first_split, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=second_split, random_state=42)\n",
    "\n",
    "# 3. Create Dataloaders\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "valid_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader,\n",
    "    'test': test_loader,\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Model\n",
    "1. Develop Deep Learning Models\n",
    "We'll create two models based on the given requirements:\n",
    "\n",
    "A model with one CNN block\n",
    "A model with two CNN blocks\n",
    "\n",
    "2. Train and Evaluate Models\n",
    "We'll use the provided training loop and modify it slightly to evaluate on the validation set. Here's a function to train and evaluate:\n",
    "\n",
    "3. Make Predictions\n",
    "Use the test function provided in the original code to make predictions on new images.\n",
    "\n",
    "4. Tweak Network's Hyper-parameters to Overfit and Underfit\n",
    "To overfit:\n",
    "\n",
    "Reduce the size of the dataset.\n",
    "Increase the complexity of the model by adding more layers or increasing the number of channels.\n",
    "Increase the number of epochs.\n",
    "Do not use any form of regularization (like dropout).\n",
    "To underfit:\n",
    "\n",
    "Use a very simple model (like only one CNN block with fewer channels).\n",
    "Use very few epochs.\n",
    "Introduce high regularization (like high dropout rates).\n",
    "To achieve overfitting or underfitting, you can play around with the model architectures and the training configurations.\n",
    "\"\"\"\n",
    "class CNN_OneBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_OneBlock, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(16*14*14, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CNN_TwoBlocks(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_TwoBlocks, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(32*7*7, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_and_evaluate(num_epochs, model, loaders, optimizer, loss_func):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_step = len(loaders['train'])\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            output = model(images)\n",
    "            loss = loss_func(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loaders['val']:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Validation Accuracy of the model: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "# Overfitting\n",
    "overfit_model = CNN_TwoBlocks()  # or another complex architecture\n",
    "optimizer = optim.Adam(overfit_model.parameters(), lr=0.01)\n",
    "train_and_evaluate(200, overfit_model, loaders, optimizer, loss_func)  # Use more epochs\n",
    "\n",
    "# Underfitting\n",
    "underfit_model = CNN_OneBlock()  # or an even simpler architecture\n",
    "optimizer = optim.Adam(underfit_model.parameters(), lr=0.01)\n",
    "train_and_evaluate(10, underfit_model, loaders, optimizer, loss_func)  # Use very few epochs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
